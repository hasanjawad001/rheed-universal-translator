# ğŸ§¬ RHEED Universal Translator  
**Universal Translation Between RHEED Images and Stoichiometry**

> ğŸ† Microscopy Hackathon 2025

---

## ğŸš€ Overview

**RHEED Universal Translator** is a deep-learning framework that enables **bi-directional translation** between:

- ğŸ“¸ **RHEED images â†’ stoichiometry**
- ğŸ§ª **Stoichiometry â†’ RHEED images**

Unlike traditional one-way predictors, this project learns a **shared latent representation** that supports both **forward** and **inverse** modeling. The entire workflow is wrapped in a **no-code Streamlit UI**, allowing users to explore predictions without writing any code.

> **âœ¨ Key idea:** This work demonstrates bidirectional inference in thin-film growth by jointly learning from **RHEED images** and **XPS-derived stoichiometry** for the **SrTiOâ‚ƒ** material system. By enforcing a shared latent representation, the approach lays the foundation for **closed-loop** and **inverse-design microscopy workflows**, and can be generalized to **arbitrary ABOâ‚ƒ oxide systems** as more data become available.

---

## ğŸ§  Scientific Background

This project builds upon the work:

> **Sumner Harris et al.**  
> *Deep learning with reflection high-energy electron diffraction images to predict cation ratio in Srâ‚‚â‚“Tiâ‚‚(1âˆ’x)Oâ‚ƒ thin films*  
> ğŸ“„ https://arxiv.org/abs/2501.18523  
> ğŸ’» https://github.com/sumner-harris/Deep-Learning-with-RHEED

The original study demonstrated **forward prediction**:

> **RHEED â†’ stoichiometry**

---

### ğŸ”„ Hackathon Extension

For **Microscopy Hackathon 2025**, we extend the original work into a **universal translator** capable of:

- **RHEED â†’ Stoichiometry**  
- **Stoichiometry â†’ RHEED**  
- **RHEED â†’ RHEED** (image reconstruction)  
- **Stoichiometry â†’ Stoichiometry** (self-consistency reconstruction)

This enables inverse generation, consistency checks, and improved interpretability across modalities, all exposed through an interactive, no-code interface.

---

## ğŸ“‚ Repository Structure

```
rheed-universal-translator/
â”œâ”€â”€ app.py                  # Demo UI (no-code interface)
â”œâ”€â”€ models.py               # Universal Translator architecture
â”œâ”€â”€ utils.py                # Utilities
â”œâ”€â”€ requirements.txt        # Reproducible environment
â”œâ”€â”€ inputs/
â”‚   â””â”€â”€ rheed_stoich_data.npz   # RHEED images + stoichiometry dataset
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ model_weights.pth       # Pretrained model weights (downloaded automatically)
â”‚   â””â”€â”€ model_meta.npz          # Metadata
â”œâ”€â”€ 1_explore_data.ipynb     # Data visualization & inspection
â”œâ”€â”€ 2_build_model.ipynb      # Full training pipeline
â”œâ”€â”€ 3_evaluate_model.ipynb   # Model evaluation
â””â”€â”€ README.md

```

---

## ğŸ“Š Dataset

The dataset used in this project is provided as:

**`inputs/rheed_stoich_data.npz`**

```python
loaded_data = np.load("inputs/rheed_stoich_data.npz")
stoich = loaded_data["stoich"].astype(np.float32)
images = loaded_data["images"].astype(np.float32)
```

- **Total samples:** 31 paired measurements  
- Each sample contains:
  - ğŸ“¸ One RHEED image  
  - ğŸ§ª One corresponding stoichiometry value  

âš ï¸ *Note:* This dataset is relatively small for deep learning models and is used primarily to demonstrate the modeling pipeline and universal translation concept. Model performance is expected to improve as more data becomes available.

---

## ğŸ—ï¸ Model Architecture

The Universal Translator is built around a shared latent-space architecture consisting of:

- Separate encoders for **RHEED images** and **stoichiometry**
- A **shared latent representation** enabling cross-modal translation
- Dual decoders for:
  - Latent â†’ RHEED image
  - Latent â†’ stoichiometry

This design enables both forward and inverse translation while enforcing self-consistency across modalities.

<p align="center">
  <img src="assets/model_archi.png" width="700">
</p>

---

## ğŸ““ Notebooks

### `1_explore_data.ipynb`
- Visualize RHEED images and inspect corresponding stoichiometry values

### `2_build_model.ipynb`
- Complete model training pipeline
- Uses an 80% training split (24 samples)
- Saves pretrained model weights and metadata

### `3_evaluate_model.ipynb`
- Demonstrates:
  - RHEED â†’ Stoichiometry prediction
  - Stoichiometry â†’ RHEED image generation
- Visualizes reconstruction and translation results

---

## ğŸ›ï¸ User Interface

The **application (`app.py`)** provides a fully **no-code interface** for interacting with the RHEED Universal Translator using **Streamlit**.

### ğŸ” RHEED â†’ Stoichiometry
- Select a RHEED image from the provided dataset **or upload a custom RHEED image**
- The model predicts:
  - the corresponding **stoichiometry**
  - a **reconstructed RHEED image** (image â†’ image)

### ğŸ” Stoichiometry â†’ RHEED
- Select a stoichiometry value from the dataset **or enter a custom value**
- The model generates:
  - a **predicted RHEED image**
  - a **reconstructed stoichiometry** (stoichiometry â†’ stoichiometry)

âš ï¸ *Note:* Custom stoichiometry inputs are restricted to the range of the training dataset to avoid unreliable extrapolation.

---

## âš¡ Installation

### Tested Environment
- **Python**: 3.11.14  
- **uv**: 0.9.11  

### 1ï¸âƒ£ Install `uv` (if not already installed)
```bash
pip install uv
```

### 2ï¸âƒ£ Create and activate a virtual environment (recommended)

**On macOS / Linux**
```bash
uv venv
source .venv/bin/activate
```

**On Windows**

**PowerShell**
```powershell
uv venv
.\.venv\Scripts\Activate.ps1
```

**Command Prompt**
```cmd
uv venv
.venv\Scripts\activate.bat
```

### 3ï¸âƒ£ Install dependencies
```bash
uv pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the application
```bash
streamlit run app.py
```

On first run, the application will automatically:

- Download the pretrained model weights (~193 MB â€” may take a few minutes depending on internet speed)
- Load model metadata and the dataset
- Launch the interactive UI (user interface)

---

## ğŸ”— Pretrained Model Weights

The pretrained model used by the Streamlit app is hosted on GitHub Releases:

ğŸ‘‰ **Download link:**  
https://github.com/hasanjawad001/rheed-universal-translator/releases/download/v1.0/model_weights.pth

- File size: ~193 MB
- Automatically downloaded by the app on first run
- Loaded in the background (no manual steps required)

---

## ğŸŒ± Potential Future Directions

- Scale to larger RHEED datasets  
- Extend to additional materials systems  
- Uncertainty-aware predictions  
- Multi-modal extensions (RHEED + XRD + Raman)
- Closed-loop experiment steering and inverse design

---

## ğŸ™Œ Acknowledgements

This work builds upon and is inspired by:

- **Sumner Harris et al.**, *Deep Learning with RHEED*  
- **Microscopy Hackathon 2025** organizers and community  

### Hackathon Contributors
This project was developed during **Microscopy Hackathon 2025** by:

- **Jawad Chowdhury** â€” Oak Ridge National Laboratory (ORNL)  
- **Asraful Haque** â€” Oak Ridge National Laboratory (ORNL)

---

## â­ Like This Project?

If you found **RHEED Universal Translator** useful or inspiring:

ğŸ‘‰ **Please give this repository a â­ on GitHub!**

Happy Hackathon! ğŸ¥³  
Letâ€™s build the future of **AI-driven microscopy** together.
